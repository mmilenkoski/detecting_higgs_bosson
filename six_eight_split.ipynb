{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.helpers import *\n",
    "from utils.preprocessing import *\n",
    "from impl.implementations import *\n",
    "import datetime\n",
    "from utils.plots import *\n",
    "from utils.cross_validation import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, tx_train, ids_train = load_csv_data('data/train.csv')\n",
    "y_train.shape = (-1, 1)\n",
    "y_test, tx_test, ids_test = load_csv_data('data/test.csv')\n",
    "y_test.shape = (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sep, y_train_sep, ids_train_sep, indx_train_sep = split_data_by_DER_mass_MMC(tx_train, y_train, ids_train)\n",
    "x_test_sep, y_test_sep, ids_test_sep, indx_test_sep = split_data_by_DER_mass_MMC(tx_test, y_test, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73790, 18)\n",
      "(26123, 17)\n",
      "(69982, 22)\n",
      "(7562, 21)\n",
      "\n",
      "(47427, 29)\n",
      "(2952, 28)\n",
      "(20687, 29)\n",
      "(1477, 28)\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(x_train_sep[i].shape)\n",
    "    if i == 3:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 73790,18\n",
      "Y shape: 73790,1\n",
      "\n",
      "X shape: 26123,17\n",
      "Y shape: 26123,1\n",
      "\n",
      "X shape: 69982,22\n",
      "Y shape: 69982,1\n",
      "\n",
      "X shape: 7562,21\n",
      "Y shape: 7562,1\n",
      "\n",
      "X shape: 68114,29\n",
      "Y shape: 68114,1\n",
      "X shape: 4429,28\n",
      "Y shape: 4429,1\n"
     ]
    }
   ],
   "source": [
    "six_x_train_sep = {}\n",
    "six_y_train_sep = {}\n",
    "for i in range(4):\n",
    "    six_x_train_sep[i] = x_train_sep[i]\n",
    "    six_y_train_sep[i] = y_train_sep[i]\n",
    "    print(\"X shape: %s,%s\" % (six_x_train_sep[i].shape))\n",
    "    print(\"Y shape: %s,%s\" % (six_y_train_sep[i].shape))\n",
    "    print()\n",
    "\n",
    "for i in [4, 5]:\n",
    "    six_x_train_sep[i] = np.concatenate((x_train_sep[i], x_train_sep[i+2]))\n",
    "    six_y_train_sep[i] = np.concatenate((y_train_sep[i], y_train_sep[i+2]))\n",
    "    print(\"X shape: %s,%s\" % (six_x_train_sep[i].shape))\n",
    "    print(\"Y shape: %s,%s\" % (six_y_train_sep[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_x_train_sep = x_train_sep\n",
    "eight_y_train_sep = y_train_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-09, 5.99484250e-09, 3.59381366e-08, 2.15443469e-07,\n",
       "       1.29154967e-06, 7.74263683e-06, 4.64158883e-05, 2.78255940e-04,\n",
       "       1.66810054e-03, 1.00000000e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = np.logspace(-9, -2, 10)\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into six datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "NEW LAMBDA: 1e-09\n",
      "Itteration: 0, Loss: 2.9532178384785808\n",
      "Itteration: 1000, Loss: 2.9050300433527974\n",
      "Itteration: 2000, Loss: 2.8575046479510737\n",
      "Itteration: 3000, Loss: 2.8106436603393643\n",
      "Itteration: 4000, Loss: 2.764448006021022\n",
      "Itteration: 0, Loss: 3.8483547094971997\n",
      "Itteration: 1000, Loss: 3.703550167520312\n",
      "Itteration: 2000, Loss: 3.5675603862258374\n",
      "Itteration: 3000, Loss: 3.439226092047553\n",
      "Itteration: 4000, Loss: 3.3172540626080194\n",
      "Itteration: 0, Loss: 3.586443375121048\n",
      "Itteration: 1000, Loss: 3.481918665098153\n",
      "Itteration: 2000, Loss: 3.3818635249956865\n",
      "Itteration: 3000, Loss: 3.286182407295225\n",
      "Itteration: 4000, Loss: 3.1944153763674015\n",
      "Itteration: 0, Loss: 1.776522091872586\n",
      "Itteration: 1000, Loss: 1.7398209827523052\n",
      "Itteration: 2000, Loss: 1.7045921997481965\n",
      "Itteration: 3000, Loss: 1.6707969881858842\n",
      "Itteration: 4000, Loss: 1.6383935942787864\n",
      "NEW LAMBDA: 5.994842503189421e-09\n",
      "Itteration: 0, Loss: 2.9529755031136378\n",
      "Itteration: 1000, Loss: 2.6740630680846427\n",
      "Itteration: 2000, Loss: 2.419317932781805\n",
      "Itteration: 3000, Loss: 2.188987034355847\n",
      "Itteration: 4000, Loss: 1.9836008606868325\n",
      "Itteration: 0, Loss: 3.84760856898735\n",
      "Itteration: 1000, Loss: 3.089513304736469\n",
      "Itteration: 2000, Loss: 2.500029729965474\n",
      "Itteration: 3000, Loss: 2.0018835893262863\n",
      "Itteration: 4000, Loss: 1.5666019739644759\n",
      "Itteration: 0, Loss: 3.58590982750931\n",
      "Itteration: 1000, Loss: 3.021568495087679\n",
      "Itteration: 2000, Loss: 2.5723411150468762\n",
      "Itteration: 3000, Loss: 2.198582451614907\n",
      "Itteration: 4000, Loss: 1.9829897761676845\n",
      "Itteration: 0, Loss: 1.776335045464119\n",
      "Itteration: 1000, Loss: 1.5775742396737626\n",
      "Itteration: 2000, Loss: 1.423616147191298\n",
      "Itteration: 3000, Loss: 1.3039941118318348\n",
      "Itteration: 4000, Loss: 1.2094604200119368\n",
      "NEW LAMBDA: 3.593813663804626e-08\n",
      "Itteration: 0, Loss: 2.9515229130330467\n",
      "Itteration: 1000, Loss: 1.6421496818562589\n",
      "Itteration: 2000, Loss: 1.0330768671798014\n",
      "Itteration: 3000, Loss: 0.7917881431268804\n",
      "Itteration: 4000, Loss: 0.6962495548864397\n",
      "Itteration: 0, Loss: 3.8431379246388704\n",
      "Itteration: 1000, Loss: 1.1258103496010816\n",
      "Itteration: 2000, Loss: 0.8383958539606539\n",
      "Itteration: 3000, Loss: 0.7326845394865876\n",
      "Itteration: 4000, Loss: 0.6699577366518752\n",
      "Itteration: 0, Loss: 3.582712530591073\n",
      "Itteration: 1000, Loss: 1.7254686493405877\n",
      "Itteration: 2000, Loss: 1.303581368215832\n",
      "Itteration: 3000, Loss: 1.0518398198303245\n",
      "Itteration: 4000, Loss: 0.8840574968662074\n",
      "Itteration: 0, Loss: 1.7752141264519015\n",
      "Itteration: 1000, Loss: 1.0698005759928118\n",
      "Itteration: 2000, Loss: 0.8417084629506056\n",
      "Itteration: 3000, Loss: 0.7310154717269671\n",
      "Itteration: 4000, Loss: 0.6658529453899125\n",
      "NEW LAMBDA: 2.1544346900318867e-07\n",
      "Itteration: 0, Loss: 2.9428210733940428\n",
      "Itteration: 1000, Loss: 0.60854826927895\n",
      "Itteration: 2000, Loss: 0.5060957584172241\n",
      "Itteration: 3000, Loss: 0.46747099217831917\n",
      "Itteration: 4000, Loss: 0.4521852576555284\n",
      "Itteration: 0, Loss: 3.816423075215987\n",
      "Itteration: 1000, Loss: 0.5927447117610631\n",
      "Itteration: 2000, Loss: 0.5023108462643673\n",
      "Itteration: 3000, Loss: 0.4769668561402537\n",
      "Itteration: 4000, Loss: 0.46630145928660355\n",
      "Itteration: 0, Loss: 3.5635896083845227\n",
      "Itteration: 1000, Loss: 0.687642609843923\n",
      "Itteration: 2000, Loss: 0.4970801318887105\n",
      "Itteration: 3000, Loss: 0.45927384327323145\n",
      "Itteration: 4000, Loss: 0.4476056474898927\n",
      "Itteration: 0, Loss: 1.768508635062968\n",
      "Itteration: 1000, Loss: 0.5894840012464597\n",
      "Itteration: 2000, Loss: 0.48898085260457297\n",
      "Itteration: 3000, Loss: 0.4529969763859927\n",
      "Itteration: 4000, Loss: 0.4400990744682699\n",
      "NEW LAMBDA: 1.2915496650148827e-06\n",
      "Itteration: 0, Loss: 2.8908819140510196\n",
      "Itteration: 1000, Loss: 0.44400025138897325\n",
      "Itteration: 2000, Loss: 0.44012196415576565\n",
      "Itteration: 3000, Loss: 0.43818890605652894\n",
      "Itteration: 4000, Loss: 0.43687445982699596\n",
      "Itteration: 0, Loss: 3.659576590884815\n",
      "Itteration: 1000, Loss: 0.45610178470178125\n",
      "Itteration: 2000, Loss: 0.4434700667655492\n",
      "Itteration: 3000, Loss: 0.438506589466182\n",
      "Itteration: 4000, Loss: 0.43616912405711955\n",
      "Itteration: 0, Loss: 3.450561010150008\n",
      "Itteration: 1000, Loss: 0.4405466110968836\n",
      "Itteration: 2000, Loss: 0.4356190893067265\n",
      "Itteration: 3000, Loss: 0.4340329860460157\n",
      "Itteration: 4000, Loss: 0.43325938714185847\n",
      "Itteration: 0, Loss: 1.7288341333347172\n",
      "Itteration: 1000, Loss: 0.433685362791133\n",
      "Itteration: 2000, Loss: 0.43155603896541117\n",
      "Itteration: 3000, Loss: 0.4309268534770237\n",
      "Itteration: 4000, Loss: 0.4305567379637795\n",
      "NEW LAMBDA: 7.742636826811277e-06\n",
      "Itteration: 0, Loss: 2.5885193409324514\n",
      "Itteration: 1000, Loss: 0.43528297279618033\n",
      "Itteration: 2000, Loss: 0.43372298966123696\n",
      "Itteration: 3000, Loss: 0.43340008833872967\n",
      "Itteration: 4000, Loss: 0.433282729969336\n",
      "Itteration: 0, Loss: 2.8359982542357054\n",
      "Itteration: 1000, Loss: 0.43419080927233483\n",
      "Itteration: 2000, Loss: 0.4329582922388365\n",
      "Itteration: 3000, Loss: 0.4327642158795437\n",
      "Itteration: 4000, Loss: 0.43268478422578643\n",
      "Itteration: 0, Loss: 2.835538382969218\n",
      "Itteration: 1000, Loss: 0.4325283552077895\n",
      "Itteration: 2000, Loss: 0.4319992810878237\n",
      "Itteration: 3000, Loss: 0.4319140602149634\n",
      "Itteration: 4000, Loss: 0.43188365059976286\n",
      "Itteration: 0, Loss: 1.5126590427392719\n",
      "Itteration: 1000, Loss: 0.43014865835637983\n",
      "Itteration: 2000, Loss: 0.4297838638260225\n",
      "Itteration: 3000, Loss: 0.4297156365949409\n",
      "Itteration: 4000, Loss: 0.4296945397244537\n",
      "0.42968670078265636\n",
      "0.42968669078599603\n",
      "NEW LAMBDA: 4.641588833612782e-05\n",
      "Itteration: 0, Loss: 1.3982841083987807\n",
      "Itteration: 1000, Loss: 1.5112159640812732\n",
      "Itteration: 2000, Loss: 1.5098407582031628\n",
      "Itteration: 3000, Loss: 1.510232312711293\n",
      "Itteration: 4000, Loss: 1.5105847883140466\n",
      "Itteration: 0, Loss: 1.5675582454387664\n",
      "Itteration: 1000, Loss: 1.8253960065306523\n",
      "Itteration: 2000, Loss: 1.1731032224567572\n",
      "Itteration: 3000, Loss: 0.8613521575114721\n",
      "Itteration: 4000, Loss: 1.5858721037625991\n",
      "Itteration: 0, Loss: 2.017703159943635\n",
      "Itteration: 1000, Loss: 0.9263886129489671\n",
      "Itteration: 2000, Loss: 1.0318874237706641\n",
      "Itteration: 3000, Loss: 1.0905777205378056\n",
      "Itteration: 4000, Loss: 0.8536396252216861\n",
      "Itteration: 0, Loss: 1.7890188733551267\n",
      "Itteration: 1000, Loss: 0.7479825871881182\n",
      "Itteration: 2000, Loss: 0.9202187973322834\n",
      "Itteration: 3000, Loss: 0.9639837199396353\n",
      "Itteration: 4000, Loss: 0.7426865737382177\n",
      "NEW LAMBDA: 0.0002782559402207126\n",
      "Itteration: 0, Loss: 8.140695403478379\n",
      "Itteration: 1000, Loss: 9.394269119158027\n",
      "Itteration: 2000, Loss: 9.400667096891482\n",
      "Itteration: 3000, Loss: 9.414224814238414\n",
      "Itteration: 4000, Loss: 9.42439459396086\n",
      "Itteration: 0, Loss: 9.095596165224817\n",
      "Itteration: 1000, Loss: 15.326281548834606\n",
      "Itteration: 2000, Loss: 3.8080021872277143\n",
      "Itteration: 3000, Loss: 6.539464140161421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c3af15d87713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msix_x_train_sep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msix_y_train_sep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoly_degree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"std\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logistic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mbest_lambdas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0macc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML EPFL/Project 1/ml_project1/utils/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(x, y, lambdas, poly_degree, norm, method, n_splits, visualize, seed, max_iters)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"logistic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0my_train_kfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_kfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train_kfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"SKL\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML EPFL/Project 1/ml_project1/impl/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# update w and get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Itteration: %s, Loss: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML EPFL/Project 1/ml_project1/impl/implementations.py\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \"\"\"\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML EPFL/Project 1/ml_project1/impl/implementations.py\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_of_models = 6\n",
    "\n",
    "best_lambdas = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "for i in range(number_of_models):\n",
    "    x_tr = six_x_train_sep[i]\n",
    "    y_tr = six_y_train_sep[i]\n",
    "    lambda_, acc_tr, acc_te = cross_validation(x_tr, y_tr, lambdas, poly_degree=2, n_splits=4, norm=\"std\", method=\"logistic\", max_iters=5000)\n",
    "    best_lambdas.append(lambda_)\n",
    "    acc_train.append(acc_tr)\n",
    "    acc_test.append(acc_te)\n",
    "    \n",
    "print(best_lambdas)\n",
    "print(np.mean(acc_train))\n",
    "print(acc_train)\n",
    "print(np.mean(acc_test))\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into eight datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_models = 8\n",
    "\n",
    "best_lambdas = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "for i in range(number_of_models):\n",
    "    x_tr = eight_x_train_sep[i]\n",
    "    y_tr = eight_y_train_sep[i]\n",
    "    lambda_, acc_tr, acc_te = cross_validation(x_tr, y_tr, lambdas, poly_degree=2, n_splits=4, norm=\"std\", method=\"logistic\", max_iters=5000)\n",
    "    best_lambdas.append(lambda_)\n",
    "    acc_train.append(acc_tr)\n",
    "    acc_test.append(acc_te)\n",
    "    \n",
    "print(best_lambdas)\n",
    "print(np.mean(acc_train))\n",
    "print(acc_train)\n",
    "print(np.mean(acc_test))\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
