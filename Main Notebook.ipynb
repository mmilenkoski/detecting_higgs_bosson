{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, tx_train, ids_train = load_csv_data('data/train.csv')\n",
    "y_test, tx_test, ids_test = load_csv_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.5000000000000002, w0=0.00010766649510400011, w1=-2.7284198603999987e-05\n",
      "Gradient Descent(1/99): loss=1.8007092524629227, w0=2.3971414374319977e-05, w1=3.523635396779801e-05\n",
      "Gradient Descent(2/99): loss=31.101752442252863, w0=0.0007710648697456105, w1=-0.00032521856702279255\n",
      "Gradient Descent(3/99): loss=688.0300820896401, w0=-0.002474078721952218, w1=0.001318897475978849\n",
      "Gradient Descent(4/99): loss=15414.27594333, w0=0.013135299159148729, w1=-0.006526189710631012\n",
      "Gradient Descent(5/99): loss=345528.428187897, w0=-0.06056322554527148, w1=0.030558075576377533\n",
      "Gradient Descent(6/99): loss=7745604.319635325, w0=0.2885459492568912, w1=-0.14508019467290945\n",
      "Gradient Descent(7/99): loss=173631018.6211872, w0=-1.3642096415793699, w1=0.6864451133452817\n",
      "Gradient Descent(8/99): loss=3892237567.6028767, w0=6.461100443635003, w1=-3.250575106470919\n",
      "Gradient Descent(9/99): loss=87251191844.92096, w0=-30.588739368440102, w1=15.389702645907896\n",
      "Gradient Descent(10/99): loss=1955885360767.4563, w0=144.82862021855436, w1=-72.86516332818053\n",
      "Gradient Descent(11/99): loss=43844530528367.73, w0=-685.7075369805966, w1=344.98888895595013\n",
      "Gradient Descent(12/99): loss=982850475704283.9, w0=3246.5736471866276, w1=-1633.3953515621165\n",
      "Gradient Descent(13/99): loss=2.2032281927779812e+16, w0=-15371.321407907706, w1=7733.521291899982\n",
      "Gradient Descent(14/99): loss=4.9389145037278765e+17, w0=72777.51669411357, w1=-36615.36008312987\n",
      "Gradient Descent(15/99): loss=1.1071425354437444e+19, w0=-344574.5862670013, w1=173360.16882438178\n",
      "Gradient Descent(16/99): loss=2.481850197778483e+20, w0=1631433.0573466136, w1=-820796.1973793986\n",
      "Gradient Descent(17/99): loss=5.563493594566242e+21, w0=-7724231.332120419, w1=3886166.0164780593\n",
      "Gradient Descent(18/99): loss=1.2471526687825605e+23, w0=36571374.73577044, w1=-18399556.86992321\n",
      "Gradient Descent(19/99): loss=2.7957069650814e+24, w0=-173151915.3738602, w1=87115087.60841012\n",
      "Gradient Descent(20/99): loss=6.267057458357858e+25, w0=819810193.4863312, w1=-412457677.25752085\n",
      "Gradient Descent(21/99): loss=1.4048685959193614e+27, w0=-3881497654.1859665, w1=1952834350.4951632\n",
      "Gradient Descent(22/99): loss=3.149254310997796e+28, w0=18377453902.34031, w1=-9245947428.675585\n",
      "Gradient Descent(23/99): loss=7.059594572863152e+29, w0=-87010438243.69196, w1=43776136891.561035\n",
      "Gradient Descent(24/99): loss=1.5825294057439432e+31, w0=411962201270.73834, w1=-207263795942.13882\n",
      "Gradient Descent(25/99): loss=3.547511537944557e+32, w0=-1950488455195.5962, w1=981317314836.5568\n",
      "Gradient Descent(26/99): loss=7.952356566754374e+33, w0=9234840483219.709, w1=-4646174060552.599\n",
      "Gradient Descent(27/99): loss=1.782657344123619e+35, w0=-43723549618221.75, w1=21997913492994.023\n",
      "Gradient Descent(28/99): loss=3.9961326933493794e+36, w0=207014814678269.03, w1=-104151973589146.16\n",
      "Gradient Descent(29/99): loss=8.958017958693238e+37, w0=-980138480760907.2, w1=493121022862871.9\n",
      "Gradient Descent(30/99): loss=2.0080936221617742e+39, w0=4640592717778782.0, w1=-2334745418733628.0\n",
      "Gradient Descent(31/99): loss=4.501486840014146e+40, w0=-2.1971487901978104e+16, w1=1.105415489822566e+16\n",
      "Gradient Descent(32/99): loss=1.0090856097140718e+42, w0=1.0402685820225053e+17, w1=-5.233732960069154e+16\n",
      "Gradient Descent(33/99): loss=2.2620387527975556e+43, w0=-4.925286478416824e+17, w1=2.477978728316091e+17\n",
      "Gradient Descent(34/99): loss=5.070748477532828e+44, w0=2.3319407423909627e+18, w1=-1.1732311573469855e+18\n",
      "Gradient Descent(35/99): loss=1.1366953855499526e+46, w0=-1.1040875794438838e+19, w1=5.554815030656582e+18\n",
      "Gradient Descent(36/99): loss=2.5480979884042964e+47, w0=5.227445796210029e+19, w1=-2.6299991976502338e+19\n",
      "Gradient Descent(37/99): loss=5.7119993984744606e+48, w0=-2.47500198907027e+20, w1=1.2452072195864451e+20\n",
      "Gradient Descent(38/99): loss=1.2804427960246807e+50, w0=1.1718217815559094e+21, w1=-5.895595029441586e+20\n",
      "Gradient Descent(39/99): loss=2.87033250446312e+51, w0=-5.54814215824003e+21, w1=2.7913459064845474e+21\n",
      "Gradient Descent(40/99): loss=6.434343425380719e+52, w0=2.626839839686969e+22, w1=-1.3215989108373388e+22\n",
      "Gradient Descent(41/99): loss=1.4423686193625833e+54, w0=-1.2437113805958354e+23, w1=6.257281396293018e+22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e6a1427858c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Start gradient descent.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgradient_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_ws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ML EPFL\\ml_project1\\implementations.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ML EPFL\\ml_project1\\implementations.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iters = 100\n",
    "gamma = 0.000001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0] * 30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y_train, tx_train, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
